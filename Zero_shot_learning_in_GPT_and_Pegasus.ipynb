{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Zero shot learning in GPT and Pegasus",
      "provenance": [],
      "authorship_tag": "ABX9TyPjqUMQfpkH9jX5bmjjwDyk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinLolochum/Natural-Language-Processing-Daily/blob/main/Zero_shot_learning_in_GPT_and_Pegasus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsun-YuLa322"
      },
      "source": [
        "**Zero-Shot learning** \r\n",
        "- Is a machine learning problem setup where at prediction time a learner observes samples that were not available during training.\r\n",
        "- Because the initial pre-training objective had some abilities to generalize, the learner can infer the class of the unobserved class.\r\n",
        "- A computer vision example is when a learner is fed with the image of a horse and a description that a zebra is like a stripped horse.\r\n",
        "- The learner is able to accurately identify a zebra during prediction even though it was not trained to do so.\r\n",
        "- In GPT, the pre-training task is one - **generating the next word**, yet GPT can be fine-tuned for a variety of tasks it was not trained for e.g sentiment analysis.\r\n",
        "- In pegasus the main pretrainig task - **Gap-sentence generation**, yet pesasus can be used for taks such as question answering, summarization etc.\r\n",
        "\r\n"
      ]
    }
  ]
}